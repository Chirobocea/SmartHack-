{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW, Adam, SGD\nimport random\n\n\nclass MyDataset:\n    def __init__(self, main_json_path, min_json_path, max_json_path):\n        # Load main JSON data\n        with open(main_json_path, 'r') as file:\n            self.data = [list(entry.values()) for entry in json.load(file)]\n\n        # Load min and max values JSON data\n        with open(min_json_path, 'r') as file:\n            self.min_values = list(json.load(file).values())\n            \n        with open(max_json_path, 'r') as file:\n            self.max_values = list(json.load(file).values())\n            \n        for entry in self.data:\n            entry = self.normalize(entry)\n\n    def normalize(self, entry):\n        for k in range(len(self.min_values)):\n            min_val = self.min_values[k]\n            max_val = self.max_values[k]\n\n            # Avoid division by zero\n            if (max_val - min_val) != 0:\n                entry[k] = (entry[k] - min_val) / (max_val - min_val)\n            else:\n                # Handle division by zero (you can choose an appropriate fallback value)\n                entry[k] = 0.0\n                \n        return entry\n\n    def denormalize(self, entry):\n        for k in range(len(self.min_values)):\n            min_val = self.min_values[k]\n            max_val = self.max_values[k]\n            if (max_val - min_val) != 0:\n                entry[k] = entry[k] * (max_val - min_val) + min_val\n            else:\n                entry[k] = min_val\n\n        return entry\n\n    def __getitem__(self, index):\n        entry = self.data[index]\n        for k in range(14):\n            if random.random() > 0.66:\n                entry[k] = 0\n        for k in range(14, len(entry), 4):\n            if random.random() > 0.66:\n                for j in range(4):\n                    entry[k+j-1] = 0\n        return torch.tensor(entry, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.data)\n\nclass Autoencoder(nn.Module):\n    def __init__(self, input_dim, bottleneck_dim, dropout_prob=0.1):\n        super(Autoencoder, self).__init__()\n\n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, input_dim // 2),\n            nn.ReLU(),\n            nn.BatchNorm1d(input_dim // 2),\n            nn.Dropout(p=dropout_prob),\n            nn.Linear(input_dim // 2, bottleneck_dim),\n            nn.ReLU(),  # Additional fully connected layer\n            nn.BatchNorm1d(bottleneck_dim),\n            nn.Dropout(p=dropout_prob)\n        )\n\n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(bottleneck_dim, input_dim // 2),\n            nn.ReLU(),\n            nn.BatchNorm1d(input_dim // 2),\n            nn.Dropout(p=dropout_prob),\n            nn.Linear(input_dim // 2, input_dim),\n            nn.ReLU(),  # Additional fully connected layer\n            nn.BatchNorm1d(input_dim),\n            nn.Dropout(p=dropout_prob),\n            nn.Sigmoid()  # Sigmoid activation to ensure outputs are in the range [0, 1]\n        )\n\n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n    \ndef train_autoencoder(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs=10, device=\"cpu\", save_path=\"best_model.pth\"):\n    model.to(device)\n    \n    best_val_loss = float('inf')  # Initialize with a large value\n    for epoch in range(num_epochs):\n        # Training\n        model.train()\n        running_loss = 0.0\n        for k, data in enumerate(train_dataloader):\n            data = data.to(device)\n            optimizer.zero_grad()\n            outputs = model(data)\n            loss = criterion(outputs, data)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            if k % 100 == 0:\n                print(f\"Training - Epoch [{epoch + 1}/{num_epochs}], Step [{k}], Loss: {loss.item():.4f}\")\n\n        # Calculate average training loss for the epoch\n        average_train_loss = running_loss / len(train_dataloader)\n        print(f\"Training - Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_train_loss:.4f}\")\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        with torch.no_grad():\n            for k, val_data in enumerate(val_dataloader):\n                val_data = val_data.to(device)\n                val_outputs = model(val_data)\n                val_loss = criterion(val_outputs, val_data)\n                val_running_loss += val_loss.item()\n\n        # Calculate average validation loss for the epoch\n        average_val_loss = val_running_loss / len(val_dataloader)\n        print(f\"Validation - Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_val_loss:.8f}\")\n\n        # Save the best model\n        if average_val_loss < best_val_loss:\n            best_val_loss = average_val_loss\n            torch.save(model.state_dict(), \"best_model.pth\")\n            print(f\"Best model saved with validation loss: {best_val_loss:.4f} at epoch {epoch + 1}\")\n\n    print(\"Training complete.\")\n\n\n\n# Training parameters\nnum_epochs = 200\nlearning_rate = 5e-4\nbatch_size = 16\nbottleneck_dim = 2\ntrain_json_path = \"/kaggle/input/parsed-veridion/train_data.json\"\nval_json_path = \"/kaggle/input/parsed-veridion/val_data.json\"\nmin_json_path = \"/kaggle/input/parsed-veridion/min_values.json\"\nmax_json_path = \"/kaggle/input/parsed-veridion/max_values.json\"\n\ntrain_dataset = MyDataset(train_json_path, min_json_path, max_json_path)\nval_dataset = MyDataset(val_json_path, min_json_path, max_json_path)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\ninput_dim = len(train_dataset.min_values)\nautoencoder = Autoencoder(input_dim, bottleneck_dim)\n\n# Define training parameters\ncriterion = nn.MSELoss()\noptimizer = Adam(autoencoder.parameters(), lr=learning_rate)\n\n# Train the autoencoder\ntrain_autoencoder(autoencoder, train_dataloader, val_dataloader, criterion, optimizer, num_epochs=num_epochs)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-12T09:37:01.293076Z","iopub.execute_input":"2023-11-12T09:37:01.293538Z","iopub.status.idle":"2023-11-12T09:40:40.202059Z","shell.execute_reply.started":"2023-11-12T09:37:01.293501Z","shell.execute_reply":"2023-11-12T09:40:40.200381Z"},"trusted":true},"execution_count":null,"outputs":[]}]}